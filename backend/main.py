from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import pandas as pd
import numpy as np
import librosa
import os
import shutil
import tempfile
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
import gc

# --- VARIABLES GLOBALES ---
DATASET_PATH = "data/dataset_audio.csv"
POPULARITY_PATH = "data/artist_popularity.csv"
DF_AUDIO = None
DF_POPULARITY = None
SCALER = None
MODEL_KNN = None
FEATURE_COLUMNS = []

# --- LIFESPAN (Gestion au d√©marrage) ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global DF_AUDIO, DF_POPULARITY, SCALER, MODEL_KNN, FEATURE_COLUMNS
    print("üîÑ D√©marrage de l'API & Entra√Ænement du mod√®le √† la vol√©e...")
    
    try:
        # 0. Gestion des chemins (Fallback local vs Docker)
        def resolve_path(path):
            if os.path.exists(path): return path
            if os.path.exists(f"../{path}"): return f"../{path}"
            return path
            
        real_dataset_path = resolve_path(DATASET_PATH)
        real_popularity_path = resolve_path(POPULARITY_PATH)

        # 1. Charger la Popularit√© (Metadata)
        if os.path.exists(real_popularity_path):
            try:
                DF_POPULARITY = pd.read_csv(real_popularity_path)
                # Cr√©ation cl√© de recherche normalis√©e
                DF_POPULARITY['search_key'] = DF_POPULARITY['search_name'].astype(str).str.lower().str.strip()
                print(f"üåü Popularit√© charg√©e : {DF_POPULARITY.shape[0]} artistes")
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur chargement popularit√© : {e}")
        else:
            print(f"‚ö†Ô∏è Fichier popularit√© introuvable ({real_popularity_path})")

        # 2. Charger le Dataset Audio & Train
        if os.path.exists(real_dataset_path):
            df = pd.read_csv(real_dataset_path)
            print(f"üìä Dataset brut charg√© : {df.shape}")

            # Nettoyage et S√©lection des Features
            # On ne garde que les colonnes num√©riques pertinentes (Moyennes + Tempo)
            cols_to_keep = [c for c in df.columns if c == 'tempo' or c.endswith('_mean')]
            
            # Filtrer le DF
            df_features = df[cols_to_keep].copy()
            df_features = df_features.dropna()
            
            # Sauvegarde la liste finale des colonnes
            FEATURE_COLUMNS = df_features.columns.tolist()

            # Entra√Ænement du Scaler
            SCALER = StandardScaler()
            X_scaled = SCALER.fit_transform(df_features)
            
            # Entra√Ænement du mod√®le KNN (10 voisins pour avoir du choix)
            MODEL_KNN = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')
            MODEL_KNN.fit(X_scaled)
            
            # Stockage du 'r√©f√©rentiel' aligned
            DF_AUDIO = df.loc[df_features.index].reset_index(drop=True)
            
            # Nettoyage RAM
            del df
            del df_features
            del X_scaled
            gc.collect()
            
            print("‚úÖ Mod√®le entra√Æn√© et pr√™t !")
        else:
            print(f"‚ùå IMPOSSIBLE de charger le dataset ({real_dataset_path}).")
        
    except Exception as e:
        print(f"‚ùå Erreur critique au d√©marrage : {e}")
    
    yield
    
    # Clean up √† l'extinction
    print("üõë Arr√™t de l'API. Nettoyage m√©moire.")
    del DF_AUDIO
    del DF_POPULARITY
    del SCALER
    del MODEL_KNN
    gc.collect()

# --- APP SETUP ---
app = FastAPI(title="Type Beat AI API (On-the-fly)", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- FONCTION D'EXTRACTION OPTIMIS√âE ---
def extract_features(audio_path):
    """
    Extrait exactement les features attendues par le mod√®le (FEATURE_COLUMNS).
    Charge max 30s de l'audio.
    """
    try:
        # 1. Chargement l√©ger (30s)
        y, sr = librosa.load(audio_path, duration=30)
        
        # Features dict
        features = {}
        
        # Tempo
        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)
        features['tempo'] = float(tempo)
        
        # Features spectrales (Moyennes)
        features['rms_mean'] = np.mean(librosa.feature.rms(y=y))
        features['spec_cent_mean'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))
        features['spec_bw_mean'] = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))
        features['spec_roll_mean'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))
        features['zcr_mean'] = np.mean(librosa.feature.zero_crossing_rate(y))
        
        # Chroma (12 notes) - Moyennes
        chroma = librosa.feature.chroma_stft(y=y, sr=sr)
        for i in range(12):
            features[f'chroma_{i}_mean'] = np.mean(chroma[i])
            
        # MFCC (20 coefficients) - Moyennes
        # Note : Le CSV peut en contenir moins (ex: 13), le filtrage final g√©rera √ßa.
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)
        for i in range(20):
            features[f'mfcc_{i}_mean'] = np.mean(mfcc[i])
            
        # 2. Alignement et Construction du Vecteur
        # On ne renvoie QUE les colonnes connues du mod√®le, dans le BON ORDRE.
        # Si une colonne manque (ex: 'spec_bw_mean' non pr√©sent dans le CSV), elle est ignor√©e.
        # Si une colonne du CSV manque ici (ex: variances), on a un probl√®me -> Mais on a filtr√© sur '_mean' au load.
        
        final_vector = []
        if FEATURE_COLUMNS is None:
             print("‚ö†Ô∏è Mod√®le non initialis√©, pas de colonnes de features.")
             return None

        for col in FEATURE_COLUMNS:
            if col in features:
                final_vector.append(features[col])
            else:
                # Fallback critique : si une feature attendue n'est pas calcul√©e
                # Pour √©viter le crash, on met 0.0, mais √ßa ne devrait pas arriver si le CSV et ce code sont synchrones.
                # print(f"‚ö†Ô∏è Feature manquante lors de l'extraction : {col}") 
                # (Comment√© pour √©viter le spam de logs si spec_bw manque)
                final_vector.append(0.0)
                
        return np.array(final_vector)

    except Exception as e:
        print(f"Erreur extraction audio: {e}")
        return None

# --- ROUTES ---

@app.get("/")
def read_root():
    return {"status": "API is running", "model_loaded": MODEL_KNN is not None}

@app.post("/predict")
async def predict_type_beat(file: UploadFile = File(...)):
    """
    Re√ßoit un fichier audio, l'analyse, et retourne les 5 beats les plus proches.
    """
    if MODEL_KNN is None:
        raise HTTPException(status_code=503, detail="Le mod√®le n'est pas encore pr√™t ou le dataset a √©chou√©.")
    
    # Cr√©ation dossier temp si besoin
    os.makedirs("temp_audio", exist_ok=True)
    
    # Chemin temporaire
    file_location = f"temp_audio/{file.filename}"
    
    try:
        # 1. Sauvegarde disque
        with open(file_location, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
            
        # 2. Extraction
        features_vector = extract_features(file_location)
        
        if features_vector is None or len(features_vector) == 0:
            raise HTTPException(status_code=400, detail="Impossible d'extraire les features audio.")
            
        # Reshape pour predict (1, n_features)
        features_vector = features_vector.reshape(1, -1)
        
        # 3. Scaling
        scaled_features = SCALER.transform(features_vector)
        
        # 4. Recherche KNN
        distances, indices = MODEL_KNN.kneighbors(scaled_features)
        
        # 5. R√©cup√©ration des R√©sultats
        recommendations = []
        
        for i, idx in enumerate(indices[0]):
            neighbor_row = DF_AUDIO.iloc[idx]
            dist = float(distances[0][i])
            
            # Gestion des champs qui peuvent manquer dans le CSV
            filename = str(neighbor_row.get('filename', 'Unknown'))
            label = str(neighbor_row.get('label', 'Unknown'))
            
            # R√©cup√©ration Stats Popularit√©
            views = 0
            popularity = 0
            
            if DF_POPULARITY is not None:
                # Recherche insensible √† la casse
                key = label.lower().strip()
                match = DF_POPULARITY[DF_POPULARITY['search_key'] == key]
                if not match.empty:
                    try:
                        views = float(match.iloc[0]['youtube_avg_views'])
                        popularity = int(match.iloc[0]['popularity'])
                    except:
                        pass # Valeurs par d√©faut

            recommendations.append({
                "rank": i + 1,
                "filename": filename,
                "label": label, 
                "distance": round(dist, 4),
                "preview_path": f"/audio/{filename}",
                "views": views,
                "popularity": popularity
            })
            
        return {
            "input_filename": file.filename,
            "recommendations": recommendations
        }

    except Exception as e:
        print(f"Erreur route predict: {e}")
        raise HTTPException(status_code=500, detail=str(e))
        
    finally:
        # Nettoyage fichier temp
        if os.path.exists(file_location):
            try:
                os.remove(file_location)
            except:
                pass

if __name__ == "__main__":
    import uvicorn
    # Pour tester en local
    uvicorn.run(app, host="0.0.0.0", port=8000)
